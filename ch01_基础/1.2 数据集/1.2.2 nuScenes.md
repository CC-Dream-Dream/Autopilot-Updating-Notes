# 1.2.2 nuScenes

本文主要介绍 nuScenes 数据集，以及其对应工具包nuscenes-devkit。本文根据官网说明、论文以及工具包源码整理归纳得到，对其他博客未提及的数据集结构、坐标系转换、传感器同步等问题进行了详细分析。

## 1.2.2.1 简介

### 一、传感器配置

nuScenes的数据采集车辆为Renault Zoe迷你电动车，配备6个周视相机，5个毫米波雷达，具备360°的视野感知能力。具体传感器信息即分布见下面的图表。

| 传感器类型 | 详细信息  |
|  ----  | ----  |
| 相机  | 6台彩色相机，1600×900的分辨率，采用JPEG格式压缩，采样频率为12Hz |
| 激光雷达  | 1台32线旋转式激光雷达，20Hz采样频率，360°水平FOV，-30°-10°的垂直FOV，探测距离70m，探测精度2cm，每秒140万点云 |
| 毫米波雷达 | 5个77GHz的毫米波雷达，FMCW调频，13Hz采样频率，探测距离250m，速度精度±0.1km/h |
| GPS和IMU | 20mm的RTK定位精度，1000Hz采样频率 |

<div align=center>
<img src="./imgs/1.2.2.1.jpg" width="600" height="350"> 
<img src="./imgs/1.2.2.2.jpg" width="350" height="350"> 
</div>
<div align=center>图1. 传感器配置 </div>

### 二、数据采集

nuScenes数据集使用了两辆传感器配置相同的雷诺电动车进行采集，采集地点为波士顿和新加坡，这两个城市以交通密集和驾驶场景复杂闻名。整个数据集包含了由人工挑选的84段log，时长约15小时，距离约242km，平均车速16km/h。数据场景覆盖了城市、住宅区、郊区、工业区各个场景，也涵盖了白天、黑夜、晴天、雨天、多云等不同时段不同天气状况。最终数据集分为1000个片段，每个片段约20s。

<div align=center>
<img src="./imgs/1.2.2.3.jpg" width="600" height="350"> 
</div>
<div align=center>图3. 数据采集俯视图 </div>

### 三、传感器同步和数据标注

nuScenes和KITTI一样，也是采用激光来控制相机的曝光时刻，不过nuScenes有6个覆盖360°视野的相机。

在nuScenes中，图像的时间戳表示相机开始曝光的时刻，激光的时间戳表示激光扫描一圈结束的时刻。当车顶激光的线束扫描到相机FOV中心区域时，便会给对应相机一个曝光信号，激光扫描一圈会触发6次相机曝光。

训练集覆盖850个场景共34149帧数据。统计结果及含义如下所示：

``` shell
Total Scene  Num: 850
Total Sample Num: 34149
[ expo_time_bet_adj_cams] Avg:    8.52ms  2STD:    8.53ms
[max_delta_time_bet_cams] Avg:   42.61ms  2STD:   42.67ms
[       cam_refresh_time] Avg:  498.93ms  2STD:  503.95ms
[     lidar_refresh_time] Avg:  498.93ms  2STD:  503.97ms
[   delta_lidar_cam_time] Avg:    0.99ms  2STD:    2.37ms
```

--------
| 统计结果 | 含义  |
|  ----  | ----  |
| expo_time_bet_adj_cams  | 相邻两个相机曝光时间差，平均8.5ms，6个相机正好50ms，符合激光的扫描频率，侧面证明了每一圈激光会触发6次相机曝光 |
| max_delta_time_bet_cams  | 6个周视相机依次曝光时刻的最大时间间隔，平均42ms，这意味着在相对40km/h相对速度下，第一个开始曝光的左前方相机和最后一个曝光的左后方相机，对同一个物体的观测距离会相差接近半米 |
| cam_refresh_time | 相机采样间隔，平均500ms，对应2Hz的关键帧采样频率 |
| lidar_refresh_time | 激光采样间隔，平均500ms，对应2Hz的关键帧采样频率 |
| delta_lidar_cam_time | 激光时间戳和左后方相机曝光时刻的差值，均值仅1ms，说明激光扫描是从左后方相机附近位置开始的 |
--------

### 四、建图定位

nuScenes的定位数据的生成分为两个阶段，首先采用离线的方式使用激光点云生成高清地图，然后在线采集数据阶段，结合里程计和激光数据，使用蒙特卡洛定位算法进行车辆定位，最终定位误差可以达到10cm以内。

## 1.2.2.2 下载

### 一、相应链接

1. [[官网] nuScenes](https://www.nuscenes.org/nuscenes?tutorial=nuscenes)
2. [[arXiv] nuScenes: A multimodal dataset for autonomous driving](https://arxiv.org/abs/1903.11027v5)
3. [[GitHub] nuScenes devkit](https://github.com/nutonomy/nuscenes-devkit)

### 二、数据集下载

1. 在[nuScenes](https://www.nuscenes.org/download)官网注册一个账号下载数据集，完整数据集包含3个部分：

    Mini：从训练/验证集抽取10个场景组成，包含完整的原始数据和标注信息，主要用于数据集的熟悉

    TrainVal：训练/验证集，包含850个场景，其中700个训练场景，150个验证场景

    Test：测试集，包含150个场景，不包含标注数据。

2. 下载好的数据集包含4个文件夹：
   
    <div align=center>
    <img src="./imgs/1.2.2.4.jpg" width="400" height="150"> 
    </div>
    <div align=center>图4. nuScenes数据集组成 </div>
    <br/>

    maps：地图数据，四张地图对应着4个数据采集地点

    samples：带有标注信息的关键帧数据，训练主要用这部分数据

    sweeps：完整时序数据，不带有标注信息，一般用于跟踪任务

    v1.0-version: 存有数据依赖关系、标注信息、标定参数的各种json文件

### 三、数据集解析

nuScenes官方提供了一个数据集开发工具nuscenes-devkit，封装了数据读取、索引、可视化等常用操作，可以直接使用pip安装：

    pip install nuscenes-devkit

这个工具包用起来还比较方便，具体怎么使用这里就不做过多介绍了，官网有详细的教程。有一点要注意的是，nuScenes解析库要求v1.0-version等4个文件夹在同一级目录，否则会无法解析。

#### (一) 数据集结构

在介绍如何使用工具包遍历数据集之前，先对nuScenes数据集结构做一个简单介绍。nuScenes数据集采用关系数据库来管理数据，数据库一共包含13张表，以json文件格式存储在./v1.0-version目录下。

    self.table_names = ['category', 'attribute', 'visibility', 'instance', 'sensor', 'calibrated_sensor','ego_pose', 'log', 'scene', 'sample', 'sample_data', 'sample_annotation', 'map']

在使用工具包获得数据集的句柄nusc后，可以通过nusc.get(table_name, token)函数来快速获取任意表中的任意数据，十分方便快捷。表和表之间的依赖关系图可以去nuScenes官网的Data format标签页中查看，这里我对官网的关系图进行了归纳和精简。

<div align=center>
<img src="./imgs/1.2.2.5.jpg" width="400" height="280"> 
</div>
<div align=center>图5. nuScenes数据关系图 </div>
<br/>

总的来说，nuScenes数据集分为mini、trainval、test三个部分，每个部分的数据结构完全相同，可以分成scene、sample、sample_data三个层级，数据访问通过token（可以理解为指针）来实现：

* scene：是一段约20s的视频片段，由于关键帧采样频率为2Hz，所以每个scene大约包含40个关键帧，可以通过scene中的pre和next来访问上下相邻的sample

* sample：对应着一个关键帧数据，存储了相机、激光雷达、毫米波雷达的token信息，mini和trainval数据集中的sample还存储了标注信息的token

* sample_data：sample中存储的token指向的数据，即我们最终真正关心的信息，比如图片路径、位姿数据、传感器标定结果、标注目标的3d信息等。获取到这些信息就可以开始训练模型了。



