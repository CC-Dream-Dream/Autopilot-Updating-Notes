# 3.1.2 2D目标检测

## 3.1.2.1 DETR

DETR的思路和传统的目标检测的本质思路有相似之处，但表现方式很不一样。传统的方法比如Anchor-based方法本质上是对预定义的密集anchors进行类别的分类和边框系数的回归。DETR则是将目标检测视为一个集合预测问题（集合和anchors的作用类似）。由于Transformer本质上是一个序列转换的作用，因此，可以将DETR视为一个从图像序列到一个集合序列的转换过程。该集合实际上就是一个可学习的位置编码（文章中也称为object queries或者output positional encoding，代码中叫作query_embed）。

<div align=center>
<img src="./imgs/3.1.2.1.jpg" width="500" height="150">
</div>
<div align=center> 图1. DETR算法流程 </div>

DETR使用的Transformer结构和原始版本稍有不同：

<div align=center>
<img src="./imgs/3.1.2.2.jpg" width="600" height="400">
</div>
<div align=center> 图2. Transformer && DETR </div>

spatial positional encoding是作者自己提出的二维空间位置编码方法，该位置编码分别被加入到了encoder的self attention和decoder的cross attention，同时object queries也被加入到了decoder的两个attention中。而原版的Transformer将位置编码加到了input和output embedding中。值得一提的是，作者在消融实验中指出即使不给encoder添加任何位置编码，最终的AP也只比完整的DETR下降了1.3个点。

代码基于PyTorch重写了TransformerEncoderLayer, TransformerDecoderLayer类，用到的PyTorch接口只有nn.MultiheadAttention类。源码需要PyTorch 1.5.0以上版本。

代码核心位于models/transformer.py和models/detr.py。

**transformer.py**

```python
class Transformer(nn.Module):
    def __init__(self, d_model=512, nhead=8, num_encoder_layers=6,
                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1,
                 activation="relu", normalize_before=False,
                 return_intermediate_dec=False):
        super().__init__()
        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward,
                                                dropout, activation, normalize_before)
        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None
        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)
        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward,
                                                dropout, activation, normalize_before)
        decoder_norm = nn.LayerNorm(d_model)
        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm,
                                          return_intermediate=return_intermediate_dec)

    def forward(self, src, mask, query_embed, pos_embed):
        # flatten NxCxHxW to HWxNxC
        bs, c, h, w = src.shape
        src = src.flatten(2).permute(2, 0, 1)
        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)
        query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)
        mask = mask.flatten(1)

        tgt = torch.zeros_like(query_embed)
        memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)
        hs = self.decoder(tgt, memory, memory_key_padding_mask=mask,
                          pos=pos_embed, query_pos=query_embed)
        return hs.transpose(1, 2), memory.permute(1, 2, 0).view(bs, c, h, w)
```
Transformer类包含了一个Encoder和一个Decoder对象。相关类的实现均可在transformer.py中找到。重点看一下forward函数，有一个对输入tensor的变换操作：**flatten NxCxHxW to HWxNxC**。

结合PyTorch中对src和tgt的形状定义可以发现，**DETR的思路是将backbone输出特征图的像素展开成一维后当成了序列长度，而batch和channel的定义不变**。故而DETR可以计算特征图的每一个像素相对于其他所有像素的相关性，这一点在CNN中是依靠感受野来实现的，可以看出Transformer能够捕获到比CNN更大的感受范围。

DETR在计算attention的时候**没有使用masked attention**，因为将特征图展开成一维以后，所有像素都可能是互相关联的，因此没必要规定mask。而src_key_padding_mask是用来将zero_pad的部分给去掉。

forward函数中有两个关键变量pos_embed和query_embed。其中pos_embed是位置编码，位于models/position_encoding.py。

**position_encoding.py**

针对二维特征图的特点，DETR实现了自己的二维位置编码方式。代码如下

```python
class PositionEmbeddingSine(nn.Module):
    """
    This is a more standard version of the position embedding, very similar to the one
    used by the Attention is all you need paper, generalized to work on images.
    """
    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):
        super().__init__()
        self.num_pos_feats = num_pos_feats
        self.temperature = temperature
        self.normalize = normalize
        if scale is not None and normalize is False:
            raise ValueError("normalize should be True if scale is passed")
        if scale is None:
            scale = 2 * math.pi
        self.scale = scale

    def forward(self, tensor_list: NestedTensor):
        x = tensor_list.tensors
        mask = tensor_list.mask
        assert mask is not None
        not_mask = ~mask
        y_embed = not_mask.cumsum(1, dtype=torch.float32)
        x_embed = not_mask.cumsum(2, dtype=torch.float32)
        if self.normalize:
            eps = 1e-6
            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale
            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale

        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)
        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)

        pos_x = x_embed[:, :, :, None] / dim_t
        pos_y = y_embed[:, :, :, None] / dim_t
        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)
        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)
        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)
        return pos
```
mask是一个位置掩码数组，对于一个没有经过zero_pad的图像，它的mask是一个全为0的数组。

Transformer原文中采用的位置编码方式为正弦编码，计算公式为:

<div align=center>
<img src="./imgs/3.1.2.3.jpg" width="400" height="100">
</div>
<div align=center> 图3. 位置编码方式为正弦编码 </div>

pos是词向量在序列中的位置，而 $i$ 是channel的index。对照代码，可以看出DETR是为二维特征图的 $x$ 和 $y$ 方向各自计算了一个位置编码，每个维度的位置编码长度为num_pos_feats（该数值实际上为hidden_dim的一半），对 $x$ 或 $y$ ，计算奇数位置的正弦，计算偶数位置的余弦，然后将pos_x和pos_y拼接起来得到一个NHWD的数组，再经过permute(0,3,1,2)，形状变为NDHW，其中D等于hidden_dim。这个hidden_dim是Transformer输入向量的维度，在实现上，**要等于CNN backbone输出的特征图的维度**。所以pos code和CNN输出特征的形状是完全一样的。

```python
src = src.flatten(2).permute(2, 0, 1)         
pos_embed = pos_embed.flatten(2).permute(2, 0, 1)
```
将CNN输出的features和pos code均进行flatten和permute操作，将形状变为SNE，符合PyTorch的输入形状定义。在TransformerEncoder中，会将src和pos_embed相加。可自行查看代码。

